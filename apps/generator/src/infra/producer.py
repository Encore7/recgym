"""
Kafka producer for streaming synthetic retail events.
"""

import logging
import signal
import sys
import time
from typing import NoReturn

from confluent_kafka import SerializingProducer
from confluent_kafka.serialization import StringSerializer

from apps.generator.src.core.config import (
    GeneratorSettings,
    KafkaSettings,
    SchemaRegistrySettings,
    get_generator_settings,
    get_kafka_settings,
    get_schema_registry_settings,
)
from apps.generator.src.infra.generator import EventGenerator
from apps.generator.src.infra.avro_serializer import build_retail_event_serializer
from apps.generator.src.infra.schema_registry import register_schemas
from apps.generator.src.infra.topics import ensure_topic
from libs.models.events import RetailEvent
from libs.observability.instrumentation import get_producer_instruments
from libs.observability.tracing import get_tracer

logger = logging.getLogger(__name__)


class KafkaEventProducer:
    """
    Kafka producer that streams events generated by EventGenerator.

    Responsibilities:
    - Bootstraps Kafka topic and Avro schemas.
    - Instantiates EventGenerator and produces events to Kafka.
    - Emits OpenTelemetry metrics for produced events and latency.
    """

    def __init__(
        self,
        generator_cfg: GeneratorSettings | None = None,
        kafka_cfg: KafkaSettings | None = None,
        schema_cfg: SchemaRegistrySettings | None = None,
    ) -> None:
        self._generator_cfg = generator_cfg or get_generator_settings()
        self._kafka_cfg = kafka_cfg or get_kafka_settings()
        self._schema_cfg = schema_cfg or get_schema_registry_settings()

        self._event_gen = EventGenerator(self._generator_cfg)

        # OTel metrics instruments (counters + histogram)
        (
            self._produced_counter,
            self._failure_counter,
            self._latency_histogram,
        ) = get_producer_instruments()

        self._running: bool = True
        self._tracer = get_tracer("recgym.generator.producer")

        # Bootstrap infra: topics + schema registry
        self._bootstrap_infra()

        # Build Avro serializer (value) and Kafka producer
        value_serializer = build_retail_event_serializer(self._schema_cfg)
        self._producer = SerializingProducer(
            {
                "bootstrap.servers": self._kafka_cfg.bootstrap_servers,
                "key.serializer": StringSerializer("utf_8"),
                "value.serializer": value_serializer,
                "compression.type": "lz4",
                "linger.ms": 50,
                "batch.size": 32_768,
            },
        )

        # Graceful shutdown via signals
        signal.signal(signal.SIGINT, self._stop)
        signal.signal(signal.SIGTERM, self._stop)

    def _bootstrap_infra(self) -> None:
        """
        Bootstrap Kafka and Schema Registry infra.

        Ensures:
        - Topic exists
        - Schemas registered
        """
        with self._tracer.start_as_current_span("bootstrap_infra"):
            try:
                ensure_topic(self._kafka_cfg)
            except Exception as exc:  # noqa: BLE001
                logger.exception(
                    "Failed to ensure Kafka topic exists.",
                    extra={"topic": self._kafka_cfg.topic},
                )
                # For local/dev we still continue; Kafka may be slow to respond.

            try:
                register_schemas(self._schema_cfg)
            except Exception as exc:  # noqa: BLE001
                logger.exception(
                    "Failed to register schemas in Schema Registry.",
                    extra={"url": self._schema_cfg.url},
                )
                # Same: log and continue, producer may still work if schemas exist.

    def _stop(self, *_: object) -> NoReturn:
        """
        Signal handler to flush pending messages and exit cleanly.
        """
        logger.info("Shutdown signal received. Flushing pending messages.")
        self._running = False
        try:
            self._producer.flush(10)
        except Exception as exc:  # noqa: BLE001
            logger.exception(
                "Error while flushing producer during shutdown.",
                extra={"error": str(exc)},
            )
        sys.exit(0)

    def _send_event(self, event: RetailEvent) -> None:
        """
        Produce a single event to Kafka with tracing + metrics.

        Args:
            event: The RetailEvent to send.
        """
        start = time.perf_counter()

        with self._tracer.start_as_current_span("produce_event") as span:
            span.set_attribute("kafka.topic", self._kafka_cfg.topic)
            span.set_attribute("event.user_id", event.user_id)
            span.set_attribute("event.item_id", event.item_id)
            span.set_attribute("event.type", event.event_type.value)

            try:
                self._producer.produce(
                    topic=self._kafka_cfg.topic,
                    key=event.user_id,
                    value=event,
                )
                self._produced_counter.add(1)
            except Exception as exc:  # noqa: BLE001
                logger.error(
                    "Produce error.",
                    extra={
                        "error": str(exc),
                        "topic": self._kafka_cfg.topic,
                        "user_id": event.user_id,
                    },
                )
                self._failure_counter.add(1)

        latency_ms = (time.perf_counter() - start) * 1000
        self._latency_histogram.record(latency_ms)
        # Trigger delivery callbacks
        self._producer.poll(0)

    def run(self) -> None:
        """
        Main producer loop â€” streams events until stopped.
        """
        logger.info(
            "KafkaEventProducer started.",
            extra={
                "topic": self._kafka_cfg.topic,
                "bootstrap_servers": self._kafka_cfg.bootstrap_servers,
            },
        )

        for event in self._event_gen.stream():
            if not self._running:
                break
            self._send_event(event)

        logger.info("Producer loop exited; flushing pending messages.")
        try:
            self._producer.flush(10)
        except Exception as exc:  # noqa: BLE001
            logger.exception(
                "Error during final producer flush.",
                extra={"error": str(exc)},
            )
